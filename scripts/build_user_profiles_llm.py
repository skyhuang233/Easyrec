# -*- coding: utf-8 -*-
"""
ç”¨æˆ·ç”»åƒç”Ÿæˆè„šæœ¬ï¼ˆDeepSeek LLMï¼Œå®Œæ•´æ›¿æ¢ç‰ˆï¼‰

åŠŸèƒ½ï¼š
1) ä» processed/train_data.csv ä¸ processed/test_pos.csv è¯»å–ç”¨æˆ·å†å²
2) æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€è¿‘ L=20 çš„è¯¾ç¨‹æ ‡é¢˜åºåˆ— + Top-2 ç±»åˆ«
3) è°ƒç”¨ DeepSeekï¼ˆé€šè¿‡ llm_utils.batch_chatï¼‰ä¸ºæ¯ä½ç”¨æˆ·ç”Ÿæˆ 3 æ¡ä¸åŒè¡¨è¿°çš„ 30~60 å­—æ‘˜è¦
4) è®­ç»ƒ/è¯„æµ‹ä¸¥æ ¼åˆ†æµï¼Œä¸è¯»å–ç­”æ¡ˆæ ‡ç­¾ï¼›è¾“å‡ºåˆ° repr/ ä¸‹çš„æ–°æ–‡ä»¶

è¾“å‡ºï¼š
- data/mooc/repr/user_profiles_train.jsonl
- data/mooc/repr/user_profiles_eval.jsonl
- data/mooc/repr/user_train_summary.cache.jsonï¼ˆç¼“å­˜ï¼‰
- data/mooc/repr/user_eval_summary.cache.jsonï¼ˆç¼“å­˜ï¼‰

å®‰å…¨ï¼š
- ä¸æ”¹åŠ¨ä»»ä½•åŸå§‹/è¯„æµ‹ç­”æ¡ˆæ–‡ä»¶
- ä»…ç”¨å†å²ä¸è¯¾ç¨‹å…ƒä¿¡æ¯

è¿è¡Œç¤ºä¾‹ï¼ˆWSLï¼‰ï¼š
export DATA_ROOT="data/mooc"
cd /mnt/d/EasyRec-main/EasyRec-main
python scripts/build_user_profiles_llm.py

è¿è¡Œç¤ºä¾‹ï¼ˆWindows PowerShellï¼‰ï¼š
$env:DATA_ROOT="data/mooc"
cd D:\EasyRec-main\EasyRec-main
python scripts/build_user_profiles_llm.py
"""

import os
import re
import json
import asyncio
from pathlib import Path
from typing import Dict, List, Tuple
from collections import Counter

import pandas as pd

# å¯¼å…¥ LLM å·¥å…·æ¨¡å—ï¼ˆç¡®ä¿ scripts/llm_utils.py å­˜åœ¨ä¸”æä¾› batch_chatï¼‰
import sys
sys.path.insert(0, os.path.dirname(__file__))
from llm_utils import batch_chat


# ==================== ç¯å¢ƒä¸å¸¸é‡ ====================
DATA_ROOT = os.environ.get("DATA_ROOT", "data/mooc")
RAW_DIR   = os.path.join(DATA_ROOT, "raw")
PROC_DIR  = os.path.join(DATA_ROOT, "processed")
REPR_DIR  = os.path.join(DATA_ROOT, "repr")

COURSES_INFO_PATH = os.path.join(RAW_DIR,  "courses_info_with_pre.csv")
TRAIN_DATA_PATH   = os.path.join(PROC_DIR, "train_data.csv")
TEST_POS_PATH     = os.path.join(PROC_DIR, "test_pos.csv")

OUT_TRAIN_JSONL   = os.path.join(REPR_DIR, "user_profiles_train.jsonl")
OUT_EVAL_JSONL    = os.path.join(REPR_DIR, "user_profiles_eval.jsonl")
CACHE_TRAIN_JSON  = os.path.join(REPR_DIR, "user_train_summary.cache.json")
CACHE_EVAL_JSON   = os.path.join(REPR_DIR, "user_eval_summary.cache.json")

# æ§åˆ¶å‚æ•°
L = 20                   # æœ€è¿‘å†å²çª—å£
MIN_INTERACTIONS = 5     # è¿‡æ»¤å†å²ä¸è¶³ç”¨æˆ·ï¼ˆä¸æ‚¨çš„æ•°æ®çº¦æŸä¸€è‡´ï¼‰
MAX_CONCURRENCY  = int(os.environ.get("MAX_CONCURRENCY", "30"))
REQUEST_TIMEOUT  = int(os.environ.get("REQUEST_TIMEOUT", "120"))
TEMPERATURE      = float(os.environ.get("TEMPERATURE", "0.2"))

# ç³»ç»Ÿæç¤ºï¼ˆæ›´ä¸¥æ ¼ï¼‰
SYSTEM_PROMPT = (
    "ä½ æ˜¯æ•™è‚²æ¨èç³»ç»Ÿçš„ç‰¹å¾å·¥ç¨‹åŠ©æ‰‹ã€‚å›ç­”éœ€å®¢è§‚ã€ç²¾ç‚¼ã€ä¸“ä¸šï¼Œä¸å¾—å£è¯­åŒ–æˆ–è¥é”€åŒ–ï¼Œç¦æ­¢è™šæ„ã€‚"
    "ä¸å¾—é€å­—å¤è¿°è¯¾ç¨‹åï¼›ä¸è¦å†™å­¦åˆ†/è¯ä¹¦/æ—¶é•¿/è€ƒè¯•ç­‰æœªç»™å‡ºçš„ä¿¡æ¯ï¼›ä¿æŒ 30~60 å­—ã€‚"
)

# è¿è§„/è™šæ„è¯è§„åˆ™ï¼ˆå¯æŒ‰éœ€å¢åˆ ï¼‰
FORBIDDEN_PATTERNS = [
    r'\d+å­¦æ—¶', r'\d+å°æ—¶', r'\d+è¯¾æ—¶', r'\d+é—¨è¯¾',
    r'\d+å­¦åˆ†', r'GPA', r'ç»©ç‚¹', r'è¯„åˆ†[:ï¼š]\d',
    r'é¢å‘è¯ä¹¦', r'å®˜æ–¹è®¤è¯', r'æƒå¨è®¤è¯',
    r'ä¿è¿‡', r'åŒ…ä¼š', r'é€šå…³', r'æœ€å…¨', r'æœ€ä½³', r'å…¨ç½‘æœ€å¥½', r'å…¨ç½‘æœ€'
]


# ==================== å·¥å…·å‡½æ•° ====================
def contains_forbidden(text: str) -> bool:
    return any(re.search(p, text) for p in FORBIDDEN_PATTERNS)

def clean_text(s: str, min_len=30, max_len=70) -> str:
    """é•¿åº¦/å­—ç¬¦æ¸…æ´— + è¿è§„è¯è¿‡æ»¤"""
    s = re.sub(r'\s+', ' ', (s or "").strip())
    if not s:
        return ""
    if contains_forbidden(s):
        return ""
    if len(s) < min_len:
        return ""
    if len(s) > max_len:
        s = s[:max_len-1] + "ã€‚"
    return s

def parse_json_array(resp: str) -> List[str]:
    """é²æ£’è§£æ JSON æ•°ç»„ï¼šç›´æ¥è§£æâ†’æ­£åˆ™æå–â†’å…œåº•å•æ¡"""
    if resp is None:
        return []
    txt = str(resp)
    try:
        arr = json.loads(txt)
        if isinstance(arr, list) and arr:
            return [str(x).strip() for x in arr if str(x).strip()]
    except Exception:
        pass
    # ä¿®æ­£å¸¸è§ä¸­æ–‡/èŠ±å¼•å·
    txt2 = (txt.replace('ï¼Œ', ',')
                .replace('â€œ', '"').replace('â€', '"')
                .replace('â€˜', '"').replace('â€™', '"'))
    m = re.search(r'\[.*\]', txt2, re.DOTALL)
    if m:
        try:
            arr = json.loads(m.group(0))
            if isinstance(arr, list) and arr:
                return [str(x).strip() for x in arr if str(x).strip()]
        except Exception:
            pass
    return [txt.strip()] if txt.strip() else []

def jaccard(a: str, b: str) -> float:
    A, B = set(a), set(b)
    return len(A & B) / len(A | B) if A or B else 1.0

def dedup_texts(texts: List[str], thr=0.70) -> List[str]:
    """å­—çº§ Jaccard å»é‡"""
    out = []
    for t in texts:
        if all(jaccard(t, x) <= thr for x in out):
            out.append(t)
    return out

def load_course_info() -> Dict[int, Dict]:
    """{course_id: {'title':..., 'category':...}}"""
    if not Path(COURSES_INFO_PATH).exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¯¾ç¨‹æ–‡ä»¶: {COURSES_INFO_PATH}")
    df = pd.read_csv(COURSES_INFO_PATH)
    info = {}
    for _, r in df.iterrows():
        cid = int(r['id'])
        info[cid] = {
            'title': str(r.get('merged_title', 'Unknown')),
            'category': str(r.get('category', 'Unknown'))
        }
    return info

def build_user_meta(history_cids: List[int], course_info: Dict) -> Tuple[str, str, int]:
    """æœ€è¿‘ L=20 çš„æ ‡é¢˜ä¸² & Top-2 ç±»åˆ«ï¼›è¿”å› (titles_str, cats_str, n_hist)"""
    seq = history_cids[-L:] if len(history_cids) > L else history_cids
    titles, cats = [], []
    for cid in seq:
        if cid in course_info:
            titles.append(course_info[cid]['title'])
            cats.append(course_info[cid]['category'])
    titles_str = "ã€".join(titles[:10]) + (f"ç­‰{len(titles)}é—¨" if len(titles) > 10 else "")
    top2 = [c for c, _ in Counter([c for c in cats if c]).most_common(2)]
    cats_str = "ã€".join(top2) if top2 else "ç»¼åˆ"
    return titles_str, cats_str, len(seq)

def build_user_prompt(titles_str: str, cats_str: str, n_hist: int) -> str:
    """ç»Ÿä¸€ä¸”ä¸¥æ ¼çš„æç¤ºè¯ï¼ˆè¦æ±‚è¾“å‡º JSON æ•°ç»„ï¼‰"""
    return (
        "ä½ æ˜¯æ•™è‚²æ¨èç³»ç»Ÿçš„ç‰¹å¾å·¥ç¨‹åŠ©æ‰‹ã€‚æ ¹æ®â€œæœ€è¿‘è¯¾ç¨‹æ ‡é¢˜åˆ—è¡¨â€å’Œâ€œTop-2 ç±»åˆ«â€ï¼Œ"
        "ç”Ÿæˆ 3 æ¡ 30~60 å­—ã€ä¸åŒè¡¨è¿°çš„ä¸€å¥è¯æ‘˜è¦ï¼Œæ¦‚æ‹¬å­¦ä¹ è€…çš„ä¸»é¢˜å…´è¶£ä¸èƒ½åŠ›å€¾å‘ã€‚"
        "è¦æ±‚ï¼šå®¢è§‚ä¸“ä¸šã€æ— å£è¯­ï¼›ä¸å¾—é€å­—å¤è¿°è¯¾ç¨‹åï¼›ç¦æ­¢è™šæ„è€ƒè¯•/å­¦åˆ†/è¯ä¹¦/æ—¶é•¿ç­‰ä¿¡æ¯ã€‚\n\n"
        f"- æœ€è¿‘è¯¾ç¨‹ï¼ˆæŒ‰æ—¶é—´ï¼Œ{n_hist} é—¨ï¼‰ï¼š{titles_str}\n"
        f"- å…³æ³¨é¢†åŸŸï¼ˆTop-2ï¼‰ï¼š{cats_str}\n\n"
        "ä»…è¾“å‡º JSON æ•°ç»„ï¼ˆå¦‚ [\"...\",\"...\",\"...\"]ï¼‰ï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚"
    )


# ==================== æ•°æ®é›†å¤„ç† ====================
async def process_dataset(
    data_path: str,
    out_jsonl: str,
    cache_json: str,
    course_info: Dict,
    dataset_name: str
):
    print(f"\n{'='*60}\nå¤„ç†{dataset_name}\n{'='*60}")
    print(f"è¾“å…¥ï¼š{data_path}\nè¾“å‡ºï¼š{out_jsonl}\nç¼“å­˜ï¼š{cache_json}")

    if not Path(data_path).exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥ï¼š{data_path}")

    # è¯»å–ä¸æ¸…æ´—
    df = pd.read_csv(data_path)
    if 'date' not in df.columns:
        raise ValueError(f"{data_path} ç¼ºå°‘ date åˆ—")
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df = df.dropna(subset=['date'])

    # courses â†’ intï¼Œè¿‡æ»¤ NaN
    df['courses'] = pd.to_numeric(df['courses'], errors='coerce').astype('Int64')
    df = df.dropna(subset=['courses'])
    df['courses'] = df['courses'].astype(int)

    # æ—¶é—´æ’åº
    df = df.sort_values(['student_id', 'date'])

    # èšåˆå†å²ï¼Œå¹¶è¿‡æ»¤æœ€å°‘äº¤äº’
    hist = df.groupby('student_id')['courses'].apply(list)
    hist = hist[hist.map(len) >= MIN_INTERACTIONS]

    # æ„é€ æç¤ºï¼ˆä¿åºï¼šæŒ‰ user_id æ’åºï¼‰
    user_ids = sorted(hist.index.tolist())
    prompts: List[str] = []
    prompt_to_uid: Dict[str, int] = {}

    for uid in user_ids:
        titles_str, cats_str, n_hist = build_user_meta(hist[uid], course_info)
        p = build_user_prompt(titles_str, cats_str, n_hist)
        prompts.append(p)
        prompt_to_uid[p] = int(uid)

    print(f"å¾…è¯·æ±‚ç”¨æˆ·æ•°ï¼š{len(prompts)}")

    # è°ƒç”¨ LLMï¼ˆå‘åå…¼å®¹ï¼šbatch_chat å¯èƒ½ä¸æ”¯æŒæŸäº›å‚æ•°ï¼‰
    batch_kwargs = dict(
        prompts=prompts,
        cache_json=cache_json,
        system=SYSTEM_PROMPT,
        temperature=TEMPERATURE,
        max_concurrency=MAX_CONCURRENCY,
    )
    # å°è¯•å¯é€‰å‚æ•°ï¼ˆå¦‚æœ llm_utils ä¸æ”¯æŒï¼Œä¼šè‡ªåŠ¨é™çº§ï¼‰
    for k, v in [('request_timeout', REQUEST_TIMEOUT), ('max_retries', 3), ('keepalive', True)]:
        try:
            batch_kwargs[k] = v
        except Exception:
            pass

    try:
        results = await batch_chat(**batch_kwargs)
    except TypeError:
        # é™çº§ï¼ˆåªä¿ç•™æœ€åŸºæœ¬å‚æ•°ï¼‰
        results = await batch_chat(
            prompts=prompts,
            cache_json=cache_json,
            system=SYSTEM_PROMPT,
            temperature=TEMPERATURE,
            max_concurrency=MAX_CONCURRENCY
        )

    # å†™ç»“æœï¼ˆä¸¥æ ¼æŒ‰ prompts é¡ºåºé˜²é”™ä½ï¼‰ï¼Œæ¸…æ´—/å»é‡/è¡¥é½
    Path(REPR_DIR).mkdir(parents=True, exist_ok=True)
    with open(out_jsonl, 'w', encoding='utf-8') as f:
        for p in prompts:
            uid = prompt_to_uid[p]
            raw = results.get(p, "")
            arr = parse_json_array(raw)

            cleaned = [clean_text(x) for x in arr if x]
            cleaned = [x for x in cleaned if x]
            cleaned = dedup_texts(cleaned, thr=0.70)

            while len(cleaned) < 3:
                cleaned.append("è¯¥å­¦ä¹ è€…åœ¨æ‰€å…³æ³¨é¢†åŸŸå‘ˆç°æŒç»­å…´è¶£ä¸è¿›é˜¶æ„æ„¿ï¼Œå…·å¤‡ç¨³å¥çš„å­¦ä¹ ä¸è¿ç§»èƒ½åŠ›ã€‚")

            for s in cleaned[:3]:
                f.write(json.dumps({"user_id": uid, "text": f"ç”¨æˆ·ç”»åƒï¼š{s}\nç”¨æˆ·IDï¼š{uid}"}, ensure_ascii=False) + "\n")

    print(f"âœ“ å·²ä¿å­˜ï¼š{out_jsonl}")


# ==================== ä¸»ç¨‹åº ====================
async def main():
    print("\n" + "="*60)
    print("ç”¨æˆ·ç”»åƒç”Ÿæˆï¼ˆDeepSeek LLMï¼Œå®Œæ•´æ›¿æ¢ç‰ˆï¼‰")
    print("="*60)
    print(f"DATA_ROOT: {DATA_ROOT}")
    print(f"å¹¶å‘: {MAX_CONCURRENCY} | è¶…æ—¶: {REQUEST_TIMEOUT}s | æ¸©åº¦: {TEMPERATURE}")
    print("="*60 + "\n")

    # è¯¾ç¨‹å…ƒæ•°æ®ï¼ˆæ ‡é¢˜/ç±»åˆ«æ˜ å°„ï¼‰
    print("ğŸ“– åŠ è½½è¯¾ç¨‹å…ƒæ•°æ® ...")
    course_info = load_course_info()
    print(f"âœ“ è¯¾ç¨‹æ•°ï¼š{len(course_info)}")

    # è®­ç»ƒé›†ï¼ˆä»… train_data.csvï¼‰
    await process_dataset(
        data_path=TRAIN_DATA_PATH,
        out_jsonl=OUT_TRAIN_JSONL,
        cache_json=CACHE_TRAIN_JSON,
        course_info=course_info,
        dataset_name="è®­ç»ƒé›†"
    )

    # è¯„æµ‹é›†ï¼ˆä»… test_pos.csvï¼Œä¸è¯»å–ç­”æ¡ˆï¼‰
    await process_dataset(
        data_path=TEST_POS_PATH,
        out_jsonl=OUT_EVAL_JSONL,
        cache_json=CACHE_EVAL_JSON,
        course_info=course_info,
        dataset_name="è¯„æµ‹é›†"
    )

    print("\nå®Œæˆï¼š")
    print(f"  è®­ç»ƒç”»åƒï¼š{OUT_TRAIN_JSONL}")
    print(f"  è¯„æµ‹ç”»åƒï¼š{OUT_EVAL_JSONL}")
    print(f"  ç¼“å­˜ï¼š{CACHE_TRAIN_JSON}, {CACHE_EVAL_JSON}")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(main())

