# -*- coding: utf-8 -*-
"""
ç”¨æˆ·ç”»åƒå¤šæ ·åŒ–ç”Ÿæˆè„šæœ¬ï¼ˆDeepSeek LLMï¼‰- ä¿®æ­£ç‰ˆ

åŠŸèƒ½ï¼š
1. è¯»å–è®­ç»ƒ/è¯„æµ‹æ•°æ®ï¼ˆtrain_data.csv, test_pos.csvï¼‰
2. ä¸ºæ¯ä¸ªç”¨æˆ·åŸºäºæœ€è¿‘ 20 é—¨è¯¾ç¨‹ç”Ÿæˆ 3 æ¡ä¸åŒè¡¨è¿°çš„ç”»åƒ
3. è¾“å‡ºï¼š
   - data/mooc/repr/user_profiles_train.div3.jsonlï¼ˆè®­ç»ƒé›†ï¼‰
   - data/mooc/repr/user_profiles_eval.div3.jsonlï¼ˆè¯„æµ‹é›†ï¼‰

ä¿®æ­£ç‚¹ï¼š
âœ“ æ—¥æœŸå­—æ®µæ­£ç¡®æ’åºï¼ˆpd.to_datetimeï¼‰
âœ“ JSON è§£ææ›´é²æ£’
âœ“ é•¿åº¦/è™šæ„è¯è¿‡æ»¤
âœ“ å˜ä½“å»é‡ï¼ˆJaccardï¼‰
âœ“ å¹¶å‘/è¶…æ—¶å¯é…ç½®
âœ“ é¡ºåºå¯å¤ç°ï¼ˆæŒ‰ user_id æ’åºï¼‰
âœ“ æ–‡ä»¶æ ¡éªŒç»Ÿè®¡

å®‰å…¨ä¸éšç§ï¼š
- âœ“ è®­ç»ƒå¤šæ ·åŒ–åªè¯»å– train_data.csv
- âœ“ è¯„æµ‹å¤šæ ·åŒ–åªè¯»å– test_pos.csvï¼ˆå†å²ï¼‰
- âœ“ ä¸ä¿®æ”¹åŸå§‹ç”»åƒæ–‡ä»¶

ä¾èµ–ï¼š
pip install openai pandas tqdm tenacity

è¿è¡Œç¤ºä¾‹ï¼ˆWSLï¼‰ï¼š
export DEEPSEEK_API_KEY="your_key"
export DATA_ROOT="data/mooc"
export MAX_CONCURRENCY=5
cd /mnt/d/EasyRec-main/EasyRec-main
python scripts/diversify_user_profiles_llm.py
"""

import os
import json
import re
import asyncio
import pandas as pd
from pathlib import Path
from typing import List, Dict, Tuple
from collections import Counter
from tqdm import tqdm

# å¯¼å…¥ LLM å·¥å…·æ¨¡å—
import sys

sys.path.insert(0, os.path.dirname(__file__))
from llm_utils import batch_chat

# ============ ç¯å¢ƒå˜é‡ ============
DATA_ROOT = os.environ.get("DATA_ROOT", "data/mooc")
RAW_DIR = os.path.join(DATA_ROOT, "raw")
PROC_DIR = os.path.join(DATA_ROOT, "processed")
REPR_DIR = os.path.join(DATA_ROOT, "repr")

# è¾“å…¥è·¯å¾„
COURSES_INFO_PATH = os.path.join(RAW_DIR, "courses_info_with_pre.csv")
TRAIN_DATA_PATH = os.path.join(PROC_DIR, "train_data.csv")
TEST_POS_PATH = os.path.join(PROC_DIR, "test_pos.csv")

# è¾“å‡ºè·¯å¾„
OUTPUT_TRAIN_JSONL = os.path.join(REPR_DIR, "user_profiles_train.div3.jsonl")
OUTPUT_EVAL_JSONL = os.path.join(REPR_DIR, "user_profiles_eval.div3.jsonl")
CACHE_TRAIN_JSON = os.path.join(REPR_DIR, "user_train_div.cache.json")
CACHE_EVAL_JSON = os.path.join(REPR_DIR, "user_eval_div.cache.json")
ERROR_LOG_TRAIN = os.path.join(REPR_DIR, "user_train_div.err.log")
ERROR_LOG_EVAL = os.path.join(REPR_DIR, "user_eval_div.err.log")

# å¯é…ç½®å‚æ•°
MAX_CONCURRENCY = int(os.environ.get("MAX_CONCURRENCY", "5"))
REQUEST_TIMEOUT = int(os.environ.get("REQUEST_TIMEOUT", "120"))
TEMPERATURE = 0.2

# ============ LLM æç¤ºè¯ ============
SYSTEM_PROMPT = """ä½ æ˜¯æ•™è‚²æ¨èç³»ç»Ÿçš„ç‰¹å¾å·¥ç¨‹åŠ©æ‰‹ã€‚å›ç­”éœ€å®¢è§‚ã€ç²¾ç‚¼ã€æ— å£è¯­ã€ç¦æ­¢è™šæ„ã€‚"""

# ============ è™šæ„è¯è¿‡æ»¤ ============
FORBIDDEN_PATTERNS = [
    r'\d+å­¦æ—¶', r'\d+å°æ—¶', r'\d+è¯¾æ—¶', r'\d+é—¨è¯¾',
    r'\d+å­¦åˆ†', r'GPA', r'ç»©ç‚¹',
    r'è€ƒè¯•', r'ä½œä¸š', r'å®éªŒæŠ¥å‘Š'
]


def contains_forbidden_content(text: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦åŒ…å«è™šæ„è¯"""
    for pattern in FORBIDDEN_PATTERNS:
        if re.search(pattern, text):
            return True
    return False


def clean_text(text: str, min_len: int = 25, max_len: int = 70) -> str:
    """
    æ–‡æœ¬æ¸…æ´—ä¸è§„æ•´
    """
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text.strip())

    # æ£€æŸ¥è™šæ„å†…å®¹
    if contains_forbidden_content(text):
        return ""

    # é•¿åº¦æ§åˆ¶
    if len(text) < min_len:
        return ""
    if len(text) > max_len:
        text = text[:max_len - 1] + "ã€‚"

    return text


def build_user_prompt(titles_str: str, cats_str: str) -> str:
    """
    æ„é€ ç”¨æˆ·ç”»åƒå¤šæ ·åŒ–æç¤ºè¯
    """
    prompt = f"""ä½ æ˜¯æ•™è‚²æ¨èç³»ç»Ÿçš„ç‰¹å¾å·¥ç¨‹åŠ©æ‰‹ã€‚æ ¹æ®"æœ€è¿‘è¯¾ç¨‹æ ‡é¢˜åˆ—è¡¨"å’Œ"Top-2 ç±»åˆ«"ï¼Œç”Ÿæˆ 3 æ¡ 30~60 å­—ã€ä¸åŒè¡¨è¿°çš„ä¸€å¥è¯å­¦ä¹ è€…å…´è¶£/èƒ½åŠ›æ‘˜è¦ã€‚é¿å…å£è¯­ï¼Œä¸å¤è¿°è¯¾ç¨‹åï¼Œç¦æ­¢è™šæ„ã€‚

è¾“å…¥ï¼š
- æœ€è¿‘è¯¾ç¨‹ï¼ˆæŒ‰æ—¶é—´ï¼‰ï¼š{titles_str}
- å…³æ³¨é¢†åŸŸï¼š{cats_str}

è¾“å‡ºï¼šä»…è¾“å‡º JSON æ•°ç»„ï¼ˆä¾‹å¦‚ï¼š["...","...","..."]ï¼‰ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å¤šä½™æ–‡å­—æˆ–è§£é‡Šã€‚"""
    return prompt


def fix_json_string(text: str) -> str:
    """ä¿®æ­£å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯"""
    text = text.replace('ï¼Œ', ',').replace('"', '"').replace('"', '"')
    text = text.replace('ã€', '[').replace('ã€‘', ']')
    text = text.replace('\n', ' ').replace('\r', '')
    return text


def parse_llm_response(response: str, fallback_count: int = 3) -> List[str]:
    """è§£æ LLM è¿”å›çš„ JSON æ•°ç»„"""
    response = fix_json_string(response)

    try:
        variants = json.loads(response)
        if isinstance(variants, list) and len(variants) > 0:
            variants = [str(v).strip() for v in variants if v]
            return variants
    except json.JSONDecodeError:
        pass

    try:
        match = re.search(r'\[.*\]', response, re.DOTALL)
        if match:
            json_str = fix_json_string(match.group(0))
            variants = json.loads(json_str)
            if isinstance(variants, list) and len(variants) > 0:
                variants = [str(v).strip() for v in variants if v]
                return variants
    except:
        pass

    return [response.strip()] if response.strip() else []


def jaccard_similarity(text1: str, text2: str) -> float:
    """è®¡ç®— Jaccard ç›¸ä¼¼åº¦ï¼ˆå­—çº§åˆ«ï¼‰"""
    set1 = set(text1)
    set2 = set(text2)
    if len(set1) == 0 and len(set2) == 0:
        return 1.0
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0.0


def deduplicate_variants(variants: List[str], threshold: float = 0.75) -> List[str]:
    """å˜ä½“å»é‡"""
    if len(variants) <= 1:
        return variants

    unique = [variants[0]]
    for i in range(1, len(variants)):
        is_duplicate = False
        for existing in unique:
            sim = jaccard_similarity(variants[i], existing)
            if sim > threshold:
                is_duplicate = True
                break
        if not is_duplicate:
            unique.append(variants[i])

    return unique


def pad_variants(variants: List[str], target_count: int = 3,
                 fallback: str = "è¯¥å­¦ä¹ è€…æ‘˜è¦ï¼šå†…å®¹å¾…å®Œå–„ã€‚") -> List[str]:
    """è¡¥é½å˜ä½“æ•°é‡"""
    while len(variants) < target_count:
        variants.append(fallback)
    return variants[:target_count]


def format_user_text(summary: str, user_id: int) -> str:
    """æ ¼å¼åŒ–ä¸ºæœ€ç»ˆçš„æ–‡æœ¬æ ¼å¼"""
    text = f"ç”¨æˆ·ç”»åƒï¼š{summary}\nç”¨æˆ·IDï¼š{user_id}"
    return text


def load_course_info() -> Dict[int, Tuple[str, str]]:
    """åŠ è½½è¯¾ç¨‹ä¿¡æ¯"""
    courses_df = pd.read_csv(COURSES_INFO_PATH)
    course_info = {}
    for _, row in courses_df.iterrows():
        course_id = int(row['id'])
        title = str(row.get('merged_title', 'Unknown'))
        category = str(row.get('category', 'Unknown'))
        course_info[course_id] = (title, category)
    return course_info


def build_user_meta(user_history: List[int], course_info: Dict,
                    max_courses: int = 20) -> Tuple[str, str]:
    """æ„å»ºç”¨æˆ·å…ƒä¿¡æ¯"""
    recent_courses = user_history[-max_courses:] if len(user_history) > max_courses else user_history

    titles = []
    categories = []

    for cid in recent_courses:
        if cid in course_info:
            title, category = course_info[cid]
            titles.append(title)
            categories.append(category)

    titles_str = "ã€".join(titles[:10])
    if len(titles) > 10:
        titles_str += f"ç­‰{len(titles)}é—¨"

    category_counter = Counter(categories)
    top_cats = [cat for cat, _ in category_counter.most_common(2)]
    cats_str = "ã€".join(top_cats) if top_cats else "ç»¼åˆ"

    return titles_str, cats_str


def validate_output(output_jsonl: str) -> Dict:
    """éªŒè¯è¾“å‡ºæ–‡ä»¶"""
    print("\nğŸ“Š éªŒè¯è¾“å‡ºæ–‡ä»¶...")

    user_count = Counter()
    nan_count = 0
    empty_count = 0
    forbidden_count = 0
    forbidden_examples = []

    with open(output_jsonl, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                user = json.loads(line)
                user_id = user['user_id']
                text = user['text']

                user_count[user_id] += 1

                if pd.isna(text) or text is None:
                    nan_count += 1
                elif not text.strip():
                    empty_count += 1
                elif contains_forbidden_content(text):
                    forbidden_count += 1
                    if len(forbidden_examples) < 5:
                        forbidden_examples.append((user_id, text[:100]))
            except:
                continue

    irregular_users = {uid: cnt for uid, cnt in user_count.items() if cnt != 3}

    stats = {
        'total_lines': sum(user_count.values()),
        'total_users': len(user_count),
        'irregular_users': len(irregular_users),
        'nan_count': nan_count,
        'empty_count': empty_count,
        'forbidden_count': forbidden_count,
        'forbidden_examples': forbidden_examples
    }

    return stats


# ============ å¤„ç†å•ä¸ªæ•°æ®é›† ============
async def process_dataset(
        data_path: str,
        output_jsonl: str,
        cache_json: str,
        error_log: str,
        course_info: Dict,
        dataset_name: str
):
    """å¤„ç†å•ä¸ªæ•°æ®é›†"""
    print(f"\n{'=' * 60}")
    print(f"å¤„ç†{dataset_name}æ•°æ®é›†")
    print(f"{'=' * 60}")
    print(f"è¾“å…¥æ–‡ä»¶: {data_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_jsonl}")
    print(f"ç¼“å­˜æ–‡ä»¶: {cache_json}")

    # 1. è¯»å–äº¤äº’æ•°æ®
    print("\nğŸ“– åŠ è½½äº¤äº’æ•°æ®...")
    if not Path(data_path).exists():
        raise FileNotFoundError(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

    data_df = pd.read_csv(data_path)
    print(f"âœ“ å…± {len(data_df)} æ¡äº¤äº’è®°å½•")

    # ä¿®æ­£ç‚¹ï¼šæ—¥æœŸå­—æ®µæ­£ç¡®æ’åº
    print("ğŸ“… è½¬æ¢æ—¥æœŸæ ¼å¼...")
    data_df['date'] = pd.to_datetime(data_df['date'], errors='coerce')

    # åˆ é™¤æ—¥æœŸç¼ºå¤±çš„è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
    before_len = len(data_df)
    data_df = data_df.dropna(subset=['date'])
    if len(data_df) < before_len:
        print(f"âš ï¸  åˆ é™¤ {before_len - len(data_df)} æ¡æ—¥æœŸç¼ºå¤±è®°å½•")

    # 2. æŒ‰ç”¨æˆ·èšåˆå†å²ï¼ˆæŒ‰æ—¥æœŸæ’åºï¼‰
    print("\nğŸ“Š èšåˆç”¨æˆ·å†å²...")
    data_df = data_df.sort_values(['student_id', 'date'])
    user_history = data_df.groupby('student_id')['courses'].apply(list).to_dict()

    # ä¿®æ­£ç‚¹ï¼šæŒ‰ user_id æ’åºï¼Œç¡®ä¿é¡ºåºå¯å¤ç°
    user_ids = sorted(user_history.keys())
    print(f"âœ“ å…± {len(user_ids)} ä¸ªç”¨æˆ·")

    # 3. æ„é€ æç¤ºè¯
    print("\nğŸ“ æ„é€ æç¤ºè¯...")
    prompts = []
    prompt_to_uid = {}

    for uid in user_ids:
        history = user_history[uid]
        titles_str, cats_str = build_user_meta(history, course_info, max_courses=20)
        prompt = build_user_prompt(titles_str, cats_str)
        prompts.append(prompt)
        prompt_to_uid[prompt] = uid

    print(f"âœ“ å…±ç”Ÿæˆ {len(prompts)} æ¡æç¤ºè¯")

    # 4. æ‰¹é‡è°ƒç”¨ LLM
    print("\nğŸ“¡ è°ƒç”¨ DeepSeek API...")
    results = await batch_chat(
        prompts=prompts,
        cache_json=cache_json,
        system=SYSTEM_PROMPT,
        temperature=TEMPERATURE,
        max_concurrency=MAX_CONCURRENCY
    )

    # 5. è§£æç»“æœå¹¶ç”Ÿæˆ JSONL
    print("\nğŸ’¾ è§£æç»“æœå¹¶ç”Ÿæˆ JSONL...")
    Path(REPR_DIR).mkdir(parents=True, exist_ok=True)

    success_count = 0
    fail_count = 0
    error_log_lines = []

    with open(output_jsonl, 'w', encoding='utf-8') as f_out:
        for prompt, response in tqdm(results.items(), desc="å¤„ç†ç»“æœ"):
            if prompt not in prompt_to_uid:
                continue

            uid = prompt_to_uid[prompt]

            # è§£æå˜ä½“
            variants = parse_llm_response(response)

            # æ¸…æ´—æ–‡æœ¬
            cleaned_variants = []
            for v in variants:
                cleaned = clean_text(v, min_len=25, max_len=70)
                if cleaned:
                    cleaned_variants.append(cleaned)

            # å»é‡
            if len(cleaned_variants) > 1:
                cleaned_variants = deduplicate_variants(cleaned_variants, threshold=0.75)

            # è¡¥é½åˆ° 3 æ¡
            if len(cleaned_variants) < 3:
                fail_count += 1
                error_msg = f"user_id={uid}, valid_variants={len(cleaned_variants)}, raw={response[:100]}"
                error_log_lines.append(error_msg)

            cleaned_variants = pad_variants(cleaned_variants, target_count=3)
            success_count += 1

            # å†™å…¥ 3 æ¡å˜ä½“
            for variant in cleaned_variants:
                text = format_user_text(variant, uid)
                profile = {"user_id": uid, "text": text}
                f_out.write(json.dumps(profile, ensure_ascii=False) + '\n')

    print(f"âœ“ å·²ä¿å­˜: {output_jsonl}")

    # 6. å†™é”™è¯¯æ—¥å¿—
    if error_log_lines:
        with open(error_log, 'w', encoding='utf-8') as f_err:
            f_err.write('\n'.join(error_log_lines))
        print(f"âš ï¸  é”™è¯¯æ—¥å¿—: {error_log} ({len(error_log_lines)} æ¡)")

    # 7. éªŒè¯è¾“å‡º
    validation_stats = validate_output(output_jsonl)

    # 8. ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print(f"{dataset_name}ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)
    print(f"æ€»ç”¨æˆ·æ•°: {len(user_ids)}")
    print(f"æˆåŠŸå¤„ç†: {success_count}")
    print(f"éƒ¨åˆ†å¤±è´¥ï¼ˆå·²è¡¥é½ï¼‰: {fail_count}")
    print(f"\næ–‡ä»¶éªŒè¯:")
    print(f"  æ€»è¡Œæ•°: {validation_stats['total_lines']}")
    print(f"  é¢„æœŸè¡Œæ•°: {len(user_ids) * 3}")
    print(f"  å¼‚å¸¸ç”¨æˆ·æ•°ï¼ˆé3è¡Œï¼‰: {validation_stats['irregular_users']}")
    print(f"  ç©ºæ–‡æœ¬: {validation_stats['empty_count']}")
    print(f"  è™šæ„è¯: {validation_stats['forbidden_count']}")
    if validation_stats['forbidden_examples']:
        print(f"\n  è™šæ„è¯ç¤ºä¾‹ï¼ˆå‰5æ¡ï¼‰:")
        for uid, text in validation_stats['forbidden_examples']:
            print(f"    user_id={uid}: {text}...")
    print(f"\nè¾“å‡ºæ–‡ä»¶: {output_jsonl}")
    print("=" * 60 + "\n")

    # 9. å±•ç¤ºå‰ 3 æ¡
    print(f"ç¤ºä¾‹è¾“å‡ºï¼ˆå‰ 3 æ¡ï¼‰ï¼š")
    with open(output_jsonl, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 3:
                break
            user = json.loads(line)
            print(f"\n[{i + 1}] user_id={user['user_id']}")
            print(f"    {user['text'][:80]}...")


# ============ ä¸»æµç¨‹ ============
async def main():
    print("\n" + "=" * 60)
    print("ç”¨æˆ·ç”»åƒå¤šæ ·åŒ–ç”Ÿæˆï¼ˆDeepSeek LLMï¼‰- ä¿®æ­£ç‰ˆ")
    print("=" * 60)
    print(f"æ•°æ®æ ¹ç›®å½•: {DATA_ROOT}")
    print(f"å¹¶å‘æ•°: {MAX_CONCURRENCY}")
    print(f"è¯·æ±‚è¶…æ—¶: {REQUEST_TIMEOUT}s")
    print("å®‰å…¨çº¦æŸï¼šè®­ç»ƒå¤šæ ·åŒ–ä¸è¯»å–ä»»ä½• eval/ç­”æ¡ˆæ•°æ®")
    print("=" * 60 + "\n")

    # 1. åŠ è½½è¯¾ç¨‹ä¿¡æ¯
    print("ğŸ“– åŠ è½½è¯¾ç¨‹å…ƒæ•°æ®...")
    course_info = load_course_info()
    print(f"âœ“ å…± {len(course_info)} é—¨è¯¾ç¨‹")

    # 2. å¤„ç†è®­ç»ƒé›†
    await process_dataset(
        data_path=TRAIN_DATA_PATH,
        output_jsonl=OUTPUT_TRAIN_JSONL,
        cache_json=CACHE_TRAIN_JSON,
        error_log=ERROR_LOG_TRAIN,
        course_info=course_info,
        dataset_name="è®­ç»ƒé›†"
    )

    # 3. å¤„ç†è¯„æµ‹é›†
    await process_dataset(
        data_path=TEST_POS_PATH,
        output_jsonl=OUTPUT_EVAL_JSONL,
        cache_json=CACHE_EVAL_JSON,
        error_log=ERROR_LOG_EVAL,
        course_info=course_info,
        dataset_name="è¯„æµ‹é›†"
    )

    # 4. æ€»ç»“
    print("\n" + "=" * 60)
    print("æ‰€æœ‰ç”¨æˆ·ç”»åƒå¤šæ ·åŒ–ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)
    print(f"è®­ç»ƒé›†: {OUTPUT_TRAIN_JSONL}")
    print(f"è¯„æµ‹é›†: {OUTPUT_EVAL_JSONL}")
    print(f"ç¼“å­˜æ–‡ä»¶: {CACHE_TRAIN_JSON}, {CACHE_EVAL_JSON}")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    asyncio.run(main())

"""
å¿«é€Ÿæ£€æŸ¥è„šæœ¬ï¼ˆéªŒè¯ç”Ÿæˆç»“æœï¼‰ï¼š
python - <<'PY'
import json
from collections import Counter

for p in ["data/mooc/repr/user_profiles_train.div3.jsonl",
          "data/mooc/repr/user_profiles_eval.div3.jsonl"]:
    print(f"\n{'='*60}\n{p}\n{'='*60}")

    # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„è¡Œæ•°
    user_count = Counter()
    with open(p, encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            user_count[obj['user_id']] += 1

    print(f"æ€»ç”¨æˆ·æ•°: {len(user_count)}")
    print(f"æ€»è¡Œæ•°: {sum(user_count.values())}")
    irregular = {uid: cnt for uid, cnt in user_count.items() if cnt != 3}
    print(f"å¼‚å¸¸ç”¨æˆ·ï¼ˆé3è¡Œï¼‰: {len(irregular)}")
    if irregular:
        print(f"  ç¤ºä¾‹: {list(irregular.items())[:5]}")

    # å±•ç¤ºå‰ 3 æ¡
    print("\nå‰ 3 æ¡:")
    with open(p, encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i >= 3: break
            print(f"[{i+1}] {json.loads(line)}")
PY

åˆå¹¶åŸºçº¿ä¸å¤šæ ·åŒ–ï¼ˆå¯é€‰ï¼‰ï¼š
# WSL:
cat data/mooc/repr/user_profiles_train.jsonl data/mooc/repr/user_profiles_train.div3.jsonl > data/mooc/repr/user_profiles_train.all.jsonl
cat data/mooc/repr/user_profiles_eval.jsonl data/mooc/repr/user_profiles_eval.div3.jsonl > data/mooc/repr/user_profiles_eval.all.jsonl

# Windows PowerShell:
Get-Content data/mooc/repr/user_profiles_train.jsonl, data/mooc/repr/user_profiles_train.div3.jsonl | Set-Content data/mooc/repr/user_profiles_train.all.jsonl
Get-Content data/mooc/repr/user_profiles_eval.jsonl, data/mooc/repr/user_profiles_eval.div3.jsonl | Set-Content data/mooc/repr/user_profiles_eval.all.jsonl
"""