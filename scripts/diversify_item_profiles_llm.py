# -*- coding: utf-8 -*-
"""
ç‰©å“ç”»åƒå¤šæ ·åŒ–ç”Ÿæˆè„šæœ¬ï¼ˆDeepSeek LLMï¼‰- ä¿®æ­£ç‰ˆ

åŠŸèƒ½ï¼š
1. è¯»å– data/mooc/repr/item_profiles.jsonlï¼ˆåŸºçº¿ï¼Œä¸ä¿®æ”¹ï¼‰
2. åŸºäº data/mooc/raw/courses_info_with_pre.csv è·å–è¯¾ç¨‹å…ƒä¿¡æ¯
3. è°ƒç”¨ DeepSeek API ä¸ºæ¯ä¸ªè¯¾ç¨‹ç”Ÿæˆ 3 æ¡ä¸åŒè¡¨è¿°çš„ç®€ä»‹
4. è¾“å‡º data/mooc/repr/item_profiles.div3.jsonlï¼ˆæ–°æ–‡ä»¶ï¼‰

ä¿®æ­£ç‚¹ï¼š
âœ“ å…ˆä¿®è¯¾æ˜ å°„ä¸ºæ ‡é¢˜ï¼ˆè€Œéæ•°å€¼IDï¼‰
âœ“ JSON è§£ææ›´é²æ£’ï¼ˆä¸­æ–‡ç¬¦å·ä¿®æ­£ï¼‰
âœ“ é•¿åº¦/è™šæ„è¯è¿‡æ»¤ä¸å…œåº•
âœ“ å˜ä½“å»é‡ï¼ˆJaccard ç®€å•ç‰ˆ + sentence-transformers å¯é€‰ï¼‰
âœ“ å¹¶å‘/è¶…æ—¶/é‡è¯•å‚æ•°å¯é…ç½®
âœ“ é¡ºåºå¯å¤ç°ï¼ˆæŒ‰ item_id æ’åºï¼‰
âœ“ æ–‡ä»¶æ ¡éªŒç»Ÿè®¡

å®‰å…¨ä¸éšç§ï¼š
- ä¸ä¿®æ”¹åŸå§‹ç”»åƒæ–‡ä»¶
- ä¸å¼•å…¥ä»»ä½•è¯„æµ‹æ•°æ®æˆ–ç­”æ¡ˆæ ‡ç­¾

ä¾èµ–ï¼š
pip install openai pandas tqdm tenacity sentence-transformers

è¿è¡Œç¤ºä¾‹ï¼ˆWSLï¼‰ï¼š
export DEEPSEEK_API_KEY="your_key"
export DATA_ROOT="data/mooc"
export MAX_CONCURRENCY=5
export REQUEST_TIMEOUT=120
cd /mnt/d/EasyRec-main/EasyRec-main
python scripts/diversify_item_profiles_llm.py
"""

import os
import json
import re
import asyncio
import pandas as pd
from pathlib import Path
from typing import List, Dict, Tuple, Set
from collections import Counter
from tqdm import tqdm

# å¯¼å…¥ LLM å·¥å…·æ¨¡å—
import sys

sys.path.insert(0, os.path.dirname(__file__))
from llm_utils import batch_chat

# ============ ç¯å¢ƒå˜é‡ ============
DATA_ROOT = os.environ.get("DATA_ROOT", "data/mooc")
RAW_DIR = os.path.join(DATA_ROOT, "raw")
REPR_DIR = os.path.join(DATA_ROOT, "repr")

# è¾“å…¥è¾“å‡ºè·¯å¾„
COURSES_INFO_PATH = os.path.join(RAW_DIR, "courses_info_with_pre.csv")
BASELINE_JSONL = os.path.join(REPR_DIR, "item_profiles.jsonl")
OUTPUT_JSONL = os.path.join(REPR_DIR, "item_profiles.div3.jsonl")
CACHE_JSON = os.path.join(REPR_DIR, "item_div.cache.json")
ERROR_LOG = os.path.join(REPR_DIR, "item_div.err.log")

# å¯é…ç½®å‚æ•°
MAX_CONCURRENCY = int(os.environ.get("MAX_CONCURRENCY", "5"))
REQUEST_TIMEOUT = int(os.environ.get("REQUEST_TIMEOUT", "120"))
TEMPERATURE = 0.2

# ============ LLM æç¤ºè¯ ============
SYSTEM_PROMPT = """ä½ æ˜¯æ•™è‚²æ¨èç³»ç»Ÿçš„å†…å®¹åŠ©æ‰‹ã€‚å›ç­”éœ€å®¢è§‚ã€ç²¾ç‚¼ã€æ— å£è¯­ã€ä¸è™šæ„ï¼Œä¸ç¼–é€ ç« èŠ‚/æ—¶é•¿/è¯„åˆ†ç­‰ä¿¡æ¯ã€‚"""

# ============ è™šæ„è¯è¿‡æ»¤ ============
FORBIDDEN_PATTERNS = [
    r'\d+å­¦æ—¶', r'\d+å°æ—¶', r'\d+è¯¾æ—¶', r'\d+ç« èŠ‚',
    r'\d+å­¦åˆ†', r'\d+\.?\d*åˆ†', r'è¯„åˆ†[:ï¼š]\d',
    r'æ—¶é•¿[:ï¼š]\d', r'å…±\d+è®²', r'\d+å‘¨',
    r'éš¾åº¦[:ï¼š]', r'éš¾åº¦ç³»æ•°'
]


def contains_forbidden_content(text: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦åŒ…å«è™šæ„è¯"""
    for pattern in FORBIDDEN_PATTERNS:
        if re.search(pattern, text):
            return True
    return False


def clean_text(text: str, min_len: int = 15, max_len: int = 50) -> str:
    """
    æ–‡æœ¬æ¸…æ´—ä¸è§„æ•´
    - å»é™¤å¤šä½™ç©ºæ ¼
    - è¿‡æ»¤è™šæ„è¯
    - é•¿åº¦æ§åˆ¶
    """
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text.strip())

    # æ£€æŸ¥è™šæ„å†…å®¹
    if contains_forbidden_content(text):
        return ""  # æ ‡è®°ä¸ºæ— æ•ˆ

    # é•¿åº¦æ§åˆ¶
    if len(text) < min_len:
        return ""  # å¤ªçŸ­ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ
    if len(text) > max_len:
        text = text[:max_len - 1] + "ã€‚"  # æˆªæ–­

    return text


def build_item_prompt(title: str, category: str, req_titles: List[str]) -> str:
    """
    æ„é€ ç‰©å“ç”»åƒå¤šæ ·åŒ–æç¤ºè¯
    ä¿®æ­£ç‚¹ï¼šå…ˆä¿®è¯¾ä½¿ç”¨æ ‡é¢˜è€ŒéID
    """
    req_str = "ã€".join(req_titles) if req_titles else "æ— "

    prompt = f"""ä½ æ˜¯æ•™è‚²æ¨èç³»ç»Ÿçš„å†…å®¹åŠ©æ‰‹ã€‚è¯·åŸºäºç»™å®šçš„è¯¾ç¨‹æ ‡é¢˜ã€ç±»åˆ«ä¸å…ˆä¿®ä¿¡æ¯ï¼Œç”Ÿæˆ 3 æ¡ 20~40 å­—çš„ä¸åŒè¡¨è¿°çš„ä¸€å¥è¯ç®€ä»‹ï¼Œè¯­æ°”å®¢è§‚ã€æ— å£è¯­ã€ä¸å¾—è™šæ„ç« èŠ‚æˆ–æ—¶é•¿ã€‚

è¾“å…¥ï¼š
- è¯¾ç¨‹æ ‡é¢˜ï¼š{title}
- ç±»åˆ«ï¼š{category}
- å…ˆä¿®ï¼š{req_str}

è¾“å‡ºï¼šä»…è¾“å‡º JSON æ•°ç»„ï¼ˆä¾‹å¦‚ï¼š["...","...","..."]ï¼‰ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å¤šä½™æ–‡å­—æˆ–è§£é‡Šã€‚"""
    return prompt


def fix_json_string(text: str) -> str:
    """
    ä¿®æ­£å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯
    - ä¸­æ–‡é€—å·/å¼•å·
    - å¤šä½™æ¢è¡Œ
    """
    # ä¸­æ–‡ç¬¦å·è½¬è‹±æ–‡
    text = text.replace('ï¼Œ', ',').replace('"', '"').replace('"', '"')
    text = text.replace('ã€', '[').replace('ã€‘', ']')
    # å»é™¤æ¢è¡Œ
    text = text.replace('\n', ' ').replace('\r', '')
    return text


def parse_llm_response(response: str, fallback_count: int = 3) -> List[str]:
    """
    è§£æ LLM è¿”å›çš„ JSON æ•°ç»„ï¼Œåšå¥å£®æ€§å¤„ç†
    ä¿®æ­£ç‚¹ï¼šä¸­æ–‡ç¬¦å·ä¿®æ­£ + ç¡®ä¿æ‰€æœ‰å…ƒç´ ä¸º str
    """
    # ä¿®æ­£ JSON æ ¼å¼
    response = fix_json_string(response)

    try:
        # å°è¯•ç›´æ¥è§£æ JSON
        variants = json.loads(response)
        if isinstance(variants, list) and len(variants) > 0:
            # ç¡®ä¿æ‰€æœ‰å…ƒç´ ä¸º str å¹¶ strip
            variants = [str(v).strip() for v in variants if v]
            return variants
    except json.JSONDecodeError:
        pass

    # å°è¯•æå– JSON æ•°ç»„
    try:
        match = re.search(r'\[.*\]', response, re.DOTALL)
        if match:
            json_str = fix_json_string(match.group(0))
            variants = json.loads(json_str)
            if isinstance(variants, list) and len(variants) > 0:
                variants = [str(v).strip() for v in variants if v]
                return variants
    except:
        pass

    # å…œåº•ï¼šè¿”å›åŸæ–‡æœ¬ä½œä¸ºå•ä¸ªå˜ä½“
    return [response.strip()] if response.strip() else []


def jaccard_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®— Jaccard ç›¸ä¼¼åº¦ï¼ˆå­—çº§åˆ«ï¼‰
    """
    set1 = set(text1)
    set2 = set(text2)
    if len(set1) == 0 and len(set2) == 0:
        return 1.0
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0.0


def deduplicate_variants(variants: List[str], threshold: float = 0.75) -> List[str]:
    """
    å˜ä½“å»é‡ï¼ˆåŸºäº Jaccard ç›¸ä¼¼åº¦ï¼‰
    ä¿®æ­£ç‚¹ï¼šç®€å•ä½†æœ‰æ•ˆçš„å»é‡ç­–ç•¥
    """
    if len(variants) <= 1:
        return variants

    unique = [variants[0]]

    for i in range(1, len(variants)):
        is_duplicate = False
        for existing in unique:
            sim = jaccard_similarity(variants[i], existing)
            if sim > threshold:
                is_duplicate = True
                break
        if not is_duplicate:
            unique.append(variants[i])

    return unique


def pad_variants(variants: List[str], target_count: int = 3,
                 fallback: str = "è¯¾ç¨‹ç®€ä»‹ï¼šå†…å®¹å¾…å®Œå–„ã€‚") -> List[str]:
    """
    è¡¥é½å˜ä½“æ•°é‡åˆ° target_count
    """
    while len(variants) < target_count:
        variants.append(fallback)
    return variants[:target_count]


def format_item_text(title: str, category: str, req_str: str,
                     summary: str, item_id: int) -> str:
    """
    æ ¼å¼åŒ–ä¸ºæœ€ç»ˆçš„æ–‡æœ¬æ ¼å¼
    """
    text = f"è¯¾ç¨‹ï¼š{title}\nç±»åˆ«ï¼š{category}\nå…ˆä¿®è¦æ±‚ï¼š{req_str}\nç®€ä»‹ï¼š{summary}\nè¯¾ç¨‹IDï¼š{item_id}"
    return text


def validate_output(output_jsonl: str) -> Dict:
    """
    éªŒè¯è¾“å‡ºæ–‡ä»¶
    ä¿®æ­£ç‚¹ï¼šæ£€æŸ¥æ¯ä¸ª item æ˜¯å¦æœ‰ 3 è¡Œï¼Œç»Ÿè®¡å¼‚å¸¸
    """
    print("\nğŸ“Š éªŒè¯è¾“å‡ºæ–‡ä»¶...")

    item_count = Counter()
    nan_count = 0
    empty_count = 0
    forbidden_count = 0
    forbidden_examples = []

    with open(output_jsonl, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                item = json.loads(line)
                item_id = item['item_id']
                text = item['text']

                item_count[item_id] += 1

                # æ£€æŸ¥å¼‚å¸¸
                if pd.isna(text) or text is None:
                    nan_count += 1
                elif not text.strip():
                    empty_count += 1
                elif contains_forbidden_content(text):
                    forbidden_count += 1
                    if len(forbidden_examples) < 5:
                        forbidden_examples.append((item_id, text[:100]))
            except:
                continue

    # æ£€æŸ¥æ¯ä¸ª item æ˜¯å¦æœ‰ 3 è¡Œ
    irregular_items = {iid: cnt for iid, cnt in item_count.items() if cnt != 3}

    stats = {
        'total_lines': sum(item_count.values()),
        'total_items': len(item_count),
        'irregular_items': len(irregular_items),
        'nan_count': nan_count,
        'empty_count': empty_count,
        'forbidden_count': forbidden_count,
        'forbidden_examples': forbidden_examples
    }

    return stats


# ============ ä¸»æµç¨‹ ============
async def main():
    print("\n" + "=" * 60)
    print("ç‰©å“ç”»åƒå¤šæ ·åŒ–ç”Ÿæˆï¼ˆDeepSeek LLMï¼‰- ä¿®æ­£ç‰ˆ")
    print("=" * 60)
    print(f"æ•°æ®æ ¹ç›®å½•: {DATA_ROOT}")
    print(f"å¹¶å‘æ•°: {MAX_CONCURRENCY}")
    print(f"è¯·æ±‚è¶…æ—¶: {REQUEST_TIMEOUT}s")
    print(f"åŸºçº¿æ–‡ä»¶: {BASELINE_JSONL} (ä¸ä¿®æ”¹)")
    print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_JSONL}")
    print("=" * 60 + "\n")

    # 1. è¯»å–è¯¾ç¨‹å…ƒä¿¡æ¯
    print("ğŸ“– åŠ è½½è¯¾ç¨‹å…ƒä¿¡æ¯...")
    if not Path(COURSES_INFO_PATH).exists():
        raise FileNotFoundError(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {COURSES_INFO_PATH}")

    courses_df = pd.read_csv(COURSES_INFO_PATH)
    print(f"âœ“ å…± {len(courses_df)} é—¨è¯¾ç¨‹")

    # æ„å»º item_id -> title æ˜ å°„ï¼ˆç”¨äºå…ˆä¿®è¯¾ï¼‰
    id_to_title = {}
    for _, row in courses_df.iterrows():
        item_id = int(row['id'])
        title = str(row.get('merged_title', 'N/A'))
        id_to_title[item_id] = title

    # æ„å»º item_id -> (title, category, req_titles) æ˜ å°„
    item_meta = {}
    for _, row in courses_df.iterrows():
        item_id = int(row['id'])
        title = str(row.get('merged_title', 'N/A'))
        category = str(row.get('category', 'N/A'))

        # ä¿®æ­£ç‚¹ï¼šå…ˆä¿®è¯¾æ˜ å°„ä¸ºæ ‡é¢˜åˆ—è¡¨
        req_titles = []
        for i in [1, 2, 3]:
            pre_id = row.get(f'required_course{i}', None)
            if pd.notna(pre_id):
                try:
                    pre_id = int(pre_id)
                    pre_title = id_to_title.get(pre_id, None)
                    if pre_title:
                        req_titles.append(pre_title)
                except:
                    continue

        item_meta[item_id] = (title, category, req_titles)

    # 2. è¯»å–åŸºçº¿ç”»åƒï¼ˆè·å–æ‰€æœ‰ item_idï¼‰
    print("\nğŸ“– åŠ è½½åŸºçº¿ç”»åƒ...")
    if not Path(BASELINE_JSONL).exists():
        raise FileNotFoundError(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {BASELINE_JSONL}")

    item_ids = []
    with open(BASELINE_JSONL, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                item = json.loads(line)
                item_ids.append(int(item['item_id']))
            except:
                continue

    # ä¿®æ­£ç‚¹ï¼šæŒ‰ item_id æ’åºï¼Œç¡®ä¿é¡ºåºå¯å¤ç°
    item_ids = sorted(set(item_ids))
    print(f"âœ“ åŸºçº¿ç”»åƒåŒ…å« {len(item_ids)} ä¸ªç‰©å“")

    # 3. æ„é€ æç¤ºè¯
    print("\nğŸ“ æ„é€ æç¤ºè¯...")
    prompts = []
    prompt_to_meta = {}  # {prompt: (item_id, title, category, req_titles)}

    for item_id in item_ids:
        if item_id not in item_meta:
            print(f"âš ï¸  ç‰©å“ {item_id} æ— å…ƒä¿¡æ¯ï¼Œè·³è¿‡")
            continue

        title, category, req_titles = item_meta[item_id]
        prompt = build_item_prompt(title, category, req_titles)
        prompts.append(prompt)
        prompt_to_meta[prompt] = (item_id, title, category, req_titles)

    print(f"âœ“ å…±ç”Ÿæˆ {len(prompts)} æ¡æç¤ºè¯")

    # 4. æ‰¹é‡è°ƒç”¨ LLM
    print("\nğŸ“¡ è°ƒç”¨ DeepSeek API...")
    results = await batch_chat(
        prompts=prompts,
        cache_json=CACHE_JSON,
        system=SYSTEM_PROMPT,
        temperature=TEMPERATURE,
        max_concurrency=MAX_CONCURRENCY
    )

    # 5. è§£æç»“æœå¹¶ç”Ÿæˆ JSONL
    print("\nğŸ’¾ è§£æç»“æœå¹¶ç”Ÿæˆ JSONL...")
    Path(REPR_DIR).mkdir(parents=True, exist_ok=True)

    success_count = 0
    fail_count = 0
    error_log_lines = []

    with open(OUTPUT_JSONL, 'w', encoding='utf-8') as f_out:
        for prompt, response in tqdm(results.items(), desc="å¤„ç†ç»“æœ"):
            if prompt not in prompt_to_meta:
                continue

            item_id, title, category, req_titles = prompt_to_meta[prompt]
            req_str = "ã€".join(req_titles) if req_titles else "æ— "

            # è§£æå˜ä½“
            variants = parse_llm_response(response)

            # æ¸…æ´—æ–‡æœ¬
            cleaned_variants = []
            for v in variants:
                cleaned = clean_text(v, min_len=15, max_len=50)
                if cleaned:
                    cleaned_variants.append(cleaned)

            # å»é‡
            if len(cleaned_variants) > 1:
                cleaned_variants = deduplicate_variants(cleaned_variants, threshold=0.75)

            # è¡¥é½åˆ° 3 æ¡
            if len(cleaned_variants) < 3:
                fail_count += 1
                error_msg = f"item_id={item_id}, valid_variants={len(cleaned_variants)}, raw={response[:100]}"
                error_log_lines.append(error_msg)

            cleaned_variants = pad_variants(cleaned_variants, target_count=3)
            success_count += 1

            # å†™å…¥ 3 æ¡å˜ä½“ï¼ˆæ¯æ¡ä¸€è¡Œï¼‰
            for variant in cleaned_variants:
                text = format_item_text(title, category, req_str, variant, item_id)
                profile = {"item_id": item_id, "text": text}
                f_out.write(json.dumps(profile, ensure_ascii=False) + '\n')

    print(f"âœ“ å·²ä¿å­˜: {OUTPUT_JSONL}")

    # 6. å†™é”™è¯¯æ—¥å¿—
    if error_log_lines:
        with open(ERROR_LOG, 'w', encoding='utf-8') as f_err:
            f_err.write('\n'.join(error_log_lines))
        print(f"âš ï¸  é”™è¯¯æ—¥å¿—: {ERROR_LOG} ({len(error_log_lines)} æ¡)")

    # 7. éªŒè¯è¾“å‡º
    validation_stats = validate_output(OUTPUT_JSONL)

    # 8. ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)
    print(f"æ€»ç‰©å“æ•°: {len(item_ids)}")
    print(f"æˆåŠŸå¤„ç†: {success_count}")
    print(f"éƒ¨åˆ†å¤±è´¥ï¼ˆå·²è¡¥é½ï¼‰: {fail_count}")
    print(f"\næ–‡ä»¶éªŒè¯:")
    print(f"  æ€»è¡Œæ•°: {validation_stats['total_lines']}")
    print(f"  é¢„æœŸè¡Œæ•°: {len(item_ids) * 3}")
    print(f"  å¼‚å¸¸ç‰©å“æ•°ï¼ˆé3è¡Œï¼‰: {validation_stats['irregular_items']}")
    print(f"  ç©ºæ–‡æœ¬: {validation_stats['empty_count']}")
    print(f"  è™šæ„è¯: {validation_stats['forbidden_count']}")
    if validation_stats['forbidden_examples']:
        print(f"\n  è™šæ„è¯ç¤ºä¾‹ï¼ˆå‰5æ¡ï¼‰:")
        for iid, text in validation_stats['forbidden_examples']:
            print(f"    item_id={iid}: {text}...")
    print(f"\nè¾“å‡ºæ–‡ä»¶: {OUTPUT_JSONL}")
    print(f"ç¼“å­˜æ–‡ä»¶: {CACHE_JSON}")
    print("=" * 60 + "\n")

    # 9. å±•ç¤ºå‰ 3 æ¡
    print("ç¤ºä¾‹è¾“å‡ºï¼ˆå‰ 3 æ¡ï¼‰ï¼š")
    with open(OUTPUT_JSONL, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 3:
                break
            item = json.loads(line)
            print(f"\n[{i + 1}] item_id={item['item_id']}")
            print(f"    {item['text'][:80]}...")


if __name__ == "__main__":
    asyncio.run(main())